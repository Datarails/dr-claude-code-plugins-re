{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datarails Finance OS API Explorer\n",
    "## Interactive Notebook - Dynamic JWT Generation\n",
    "\n",
    "**Environment:** Production (app.datarails.com)  \n",
    "**Table:** Financials (ID: 16528)  \n",
    "**Records:** 54,390 total  \n",
    "**Auth:** Dynamic JWT (auto-refreshes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate Fresh JWT from sessionID & csrftoken"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T13:38:34.895443Z",
     "start_time": "2026-02-04T13:38:34.646603Z"
    }
   },
   "source": [
    "import asyncio\n",
    "import httpx\n",
    "import time\n",
    "import json\n",
    "import keyring\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "BASE_URL = \"https://app.datarails.com\"\n",
    "TABLE_ID = \"16528\"\n",
    "session_id = \"1tqttfzz17aq08ssvdij2xp02ikm1lqj\"\n",
    "csrf_token = \"PHwKH3JTIt0W5NVfK5YFiO1MemEr2wyxjDhid85WeJItjtaWJpBVkafA6Nuz6iHK\"\n",
    "\n",
    "async def get_fresh_jwt(session_id,csrf_token):\n",
    "    \"\"\"Get fresh JWT token - same logic as the MCP, just the POST call\"\"\"\n",
    "    # print(\"=\"*80)\n",
    "    # print(\"GENERATING FRESH JWT TOKEN\")\n",
    "    # print(\"=\"*80)\n",
    "    # print()\n",
    "\n",
    "    try:\n",
    "        # Load session cookies from keyring\n",
    "        # stored = keyring.get_password(KEYRING_SERVICE, KEYRING_ACCOUNT)\n",
    "        # if not stored:\n",
    "        #     print(f\"‚ùå No credentials in keyring. Run: /dr-auth --env app\")\n",
    "        #     return None, None\n",
    "\n",
    "        # data = json.loads(stored)\n",
    "        # session_id = \"1tqttfzz17aq08ssvdij2xp02ikm1lqj\"\n",
    "        # csrf_token = \"PHwKH3JTIt0W5NVfK5YFiO1MemEr2wyxjDhid85WeJItjtaWJpBVkafA6Nuz6iHK\"\n",
    "\n",
    "        # print(f\"‚úì Loaded from keyring\")\n",
    "        # print(f\"  Session ID: {session_id[:30]}...\")\n",
    "        # print(f\"  CSRF Token: {csrf_token[:30]}...\")\n",
    "        # print()\n",
    "\n",
    "        # Make the POST request to get JWT (exactly what the MCP does)\n",
    "        url = f\"{BASE_URL}/jwt/api/token/\"\n",
    "        headers = {\n",
    "            \"Cookie\": f\"csrftoken={csrf_token}; sessionid={session_id}\",\n",
    "            \"X-CSRFToken\": csrf_token,\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        }\n",
    "\n",
    "        # print(f\"Making POST request to: {url}\")\n",
    "        async with httpx.AsyncClient(timeout=30.0) as client:\n",
    "            response = await client.post(url, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            token_data = response.json()\n",
    "            access_token = token_data.get('access')\n",
    "            refresh_token = token_data.get('refresh')\n",
    "\n",
    "            if access_token:\n",
    "                # print(f\"‚úÖ JWT Generated!\")\n",
    "                # print(f\"Token: {access_token[:60]}...\")\n",
    "                # print()\n",
    "                return access_token, csrf_token\n",
    "            else:\n",
    "                print(f\"‚ùå No 'access' token in response: {token_data.keys()}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Status {response.status_code}: {response.text[:200]}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "    return None, None\n",
    "\n",
    "JWT_TOKEN, CSRF_TOKEN = await get_fresh_jwt(session_id,csrf_token)\n",
    "\n",
    "if JWT_TOKEN:\n",
    "    print(\"=\"*80)\n",
    "    print(\"‚úÖ READY TO USE\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è JWT generation failed.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "‚úÖ READY TO USE\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Client"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T13:38:38.910497Z",
     "start_time": "2026-02-04T13:38:38.903608Z"
    }
   },
   "source": [
    "class DatarailsAPI:\n",
    "    \"\"\"Async client for Datarails Finance OS API\"\"\"\n",
    "    \n",
    "    def __init__(self, jwt_token: str, csrf_token: str, base_url: str, table_id: str):\n",
    "        self.jwt_token = jwt_token\n",
    "        self.csrf_token = csrf_token\n",
    "        self.base_url = base_url\n",
    "        self.table_id = table_id\n",
    "    \n",
    "    def _get_headers(self) -> Dict:\n",
    "        \"\"\"Generate request headers with JWT Bearer token\"\"\"\n",
    "        return {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {self.jwt_token}\",\n",
    "            \"X-CSRFToken\": self.csrf_token\n",
    "        }\n",
    "    \n",
    "    async def fetch_data(\n",
    "        self, \n",
    "        limit: int = 100, \n",
    "        offset: int = 0,\n",
    "        filters: Optional[List[Dict]] = None\n",
    "    ) -> Tuple[List, Dict]:\n",
    "        \"\"\"Fetch data from API\"\"\"\n",
    "        if filters is None:\n",
    "            filters = [\n",
    "                {\"name\": \"Scenario\", \"values\": [\"Actuals\"], \"is_excluded\": False},\n",
    "                {\"name\": \"System_Year\", \"values\": [\"2025\"], \"is_excluded\": False},\n",
    "                {\"name\": \"DR_ACC_L0\", \"values\": [\"P&L\"], \"is_excluded\": False},\n",
    "            ]\n",
    "        \n",
    "        payload = {\n",
    "            \"filters\": filters,\n",
    "            \"limit\": limit,\n",
    "            \"offset\": offset\n",
    "        }\n",
    "        \n",
    "        url = f\"{self.base_url}/finance-os/api/tables/v1/{self.table_id}/data\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            async with httpx.AsyncClient(timeout=60) as client:\n",
    "                response = await client.post(url, json=payload, headers=self._get_headers())\n",
    "        except Exception as e:\n",
    "            return [], {\"status_code\": 500, \"error\": str(e), \"elapsed_seconds\": 0}\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        metadata = {\n",
    "            \"status_code\": response.status_code,\n",
    "            \"elapsed_seconds\": elapsed,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            records = data.get(\"data\", [])\n",
    "            metadata[\"records_returned\"] = len(records)\n",
    "            return records, metadata\n",
    "        else:\n",
    "            metadata[\"error\"] = response.text[:300]\n",
    "            return [], metadata\n",
    "\n",
    "# Initialize API client\n",
    "if JWT_TOKEN:\n",
    "    api = DatarailsAPI(JWT_TOKEN, CSRF_TOKEN, BASE_URL, TABLE_ID)\n",
    "    print(\"‚úì API client initialized\")\nelse:\n",
    "    print(\"‚ùå Cannot initialize - JWT not generated\")\n",
    "    api = None"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì API client initialized\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T13:38:42.308917Z",
     "start_time": "2026-02-04T13:38:42.306515Z"
    }
   },
   "cell_type": "code",
   "source": "async def init_creds(session_id, csrf_token):\n    \"\"\"Refresh JWT token and reinitialize API client with fresh credentials\"\"\"\n    global JWT_TOKEN, CSRF_TOKEN, api\n\n    JWT_TOKEN, CSRF_TOKEN = await get_fresh_jwt(session_id, csrf_token)\n\n    if JWT_TOKEN:\n        api = DatarailsAPI(JWT_TOKEN, CSRF_TOKEN, BASE_URL, TABLE_ID)\n        print(\"‚úì Credentials refreshed and API client reinitialized\")\n        return api\n    else:\n        print(\"‚ùå Failed to refresh credentials\")\n        return None",
   "outputs": [],
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Simple Request"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T13:38:50.031302Z",
     "start_time": "2026-02-04T13:38:46.942260Z"
    }
   },
   "source": [
    "async def test_1():\n",
    "    await init_creds(session_id, csrf_token)\n",
    "\n",
    "    if not api:\n",
    "        print(\"‚ùå API not initialized\")\n",
    "        return\n",
    "    \n",
    "    records, metadata = await api.fetch_data(limit=100, offset=0)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"TEST 1: SIMPLE REQUEST (100 records)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Status Code: {metadata['status_code']}\")\n",
    "    \n",
    "    if metadata['status_code'] == 200:\n",
    "        print(f\"‚úì Records: {metadata['records_returned']}\")\n",
    "        print(f\"‚úì Time: {metadata['elapsed_seconds']:.2f}s\")\n",
    "        print(f\"‚úì Rate: {metadata['records_returned'] / metadata['elapsed_seconds']:.1f} rec/sec\")\n",
    "        \n",
    "        if records:\n",
    "            print(f\"\\nFirst record:\")\n",
    "            r = records[0]\n",
    "            print(f\"  Amount: ${r.get('Amount'):,.2f}\")\n",
    "            print(f\"  Account: {r.get('DR_ACC_L1')}\")\n",
    "            print(f\"  Date: {datetime.fromtimestamp(r.get('Reporting Date')).strftime('%Y-%m-%d')}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error: {metadata.get('error')}\")\n",
    "    \n",
    "    return records\n",
    "\n",
    "sample_records = await test_1()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Credentials refreshed and API client reinitialized\n",
      "================================================================================\n",
      "TEST 1: SIMPLE REQUEST (100 records)\n",
      "================================================================================\n",
      "Status Code: 200\n",
      "‚úì Records: 100\n",
      "‚úì Time: 2.78s\n",
      "‚úì Rate: 36.0 rec/sec\n",
      "\n",
      "First record:\n",
      "  Amount: $-22.12\n",
      "  Account: Financial Expenses\n",
      "  Date: 2025-09-30\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Batch Size Performance"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T13:39:07.134252Z",
     "start_time": "2026-02-04T13:38:55.487757Z"
    }
   },
   "source": [
    "async def test_2():\n",
    "    await init_creds(session_id, csrf_token)\n",
    "\n",
    "    if not api:\n",
    "        print(\"‚ùå API not initialized\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"TEST 2: BATCH SIZE PERFORMANCE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\n{'Batch Size':<15} {'Records':<12} {'Time (sec)':<15} {'Records/sec':<15}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for batch_size in [50, 100, 250, 500, 1000, 5000]:\n",
    "        records, metadata = await api.fetch_data(limit=batch_size, offset=0)\n",
    "        if metadata['status_code'] == 200:\n",
    "            rate = metadata['records_returned'] / metadata['elapsed_seconds']\n",
    "            print(f\"{batch_size:<15} {metadata['records_returned']:<12} {metadata['elapsed_seconds']:<15.2f} {rate:<15.1f}\")\n",
    "\n",
    "await test_2()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Credentials refreshed and API client reinitialized\n",
      "================================================================================\n",
      "TEST 2: BATCH SIZE PERFORMANCE\n",
      "================================================================================\n",
      "\n",
      "Batch Size      Records      Time (sec)      Records/sec    \n",
      "--------------------------------------------------------------------------------\n",
      "50              50           1.88            26.5           \n",
      "100             100          2.18            45.8           \n",
      "250             250          2.15            116.4          \n",
      "500             500          2.44            205.3          \n",
      "1000            1000         2.38            420.3          \n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Pagination - Fetch 3000 Records"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T13:28:25.996256Z",
     "start_time": "2026-02-04T13:28:04.226990Z"
    }
   },
   "source": [
    "async def test_3():\n",
    "    await init_creds(session_id, csrf_token)\n",
    "\n",
    "    if not api:\n",
    "        print(\"‚ùå API not initialized\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"TEST 3: PAGINATION (first 3000 records)\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    \n",
    "    all_records = []\n",
    "    offset = 0\n",
    "    batch_size = 500\n",
    "    batch_num = 0\n",
    "    total_time = 0\n",
    "    \n",
    "    print(f\"{'Batch':<8} {'Offset':<8} {'Got':<8} {'Total':<8} {'Time':<8} {'Rate':<12}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    while len(all_records) < 3000:\n",
    "        batch_num += 1\n",
    "        records, metadata = await api.fetch_data(limit=batch_size, offset=offset)\n",
    "        \n",
    "        if metadata['status_code'] != 200:\n",
    "            print(f\"‚ùå Error: {metadata.get('error')}\")\n",
    "            break\n",
    "        \n",
    "        all_records.extend(records)\n",
    "        total_time += metadata['elapsed_seconds']\n",
    "        rate = len(records) / metadata['elapsed_seconds'] if metadata['elapsed_seconds'] > 0 else 0\n",
    "        \n",
    "        print(f\"{batch_num:<8} {offset:<8} {len(records):<8} {len(all_records):<8} {metadata['elapsed_seconds']:<8.2f} {rate:<12.1f}\")\n",
    "        \n",
    "        if len(records) < batch_size:\n",
    "            break\n",
    "        \n",
    "        offset += batch_size\n",
    "    \n",
    "    print(\"-\"*80)\n",
    "    if total_time > 0:\n",
    "        print(f\"\\nTotal: {len(all_records)} records in {total_time:.2f}s ({len(all_records)/total_time:.1f} rec/sec)\")\n",
    "    \n",
    "    return all_records\n",
    "\n",
    "sample = await test_3()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GENERATING FRESH JWT TOKEN\n",
      "================================================================================\n",
      "\n",
      "‚úì Loaded from keyring\n",
      "  Session ID: 1tqttfzz17aq08ssvdij2xp02ikm1l...\n",
      "  CSRF Token: PHwKH3JTIt0W5NVfK5YFiO1MemEr2w...\n",
      "\n",
      "Making POST request to: https://app.datarails.com/jwt/api/token/\n",
      "‚úÖ JWT Generated!\n",
      "Token: eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ0b2tlbl90eXBlIjoiYWN...\n",
      "\n",
      "‚úì Credentials refreshed and API client reinitialized\n",
      "================================================================================\n",
      "TEST 3: PAGINATION (first 3000 records)\n",
      "================================================================================\n",
      "\n",
      "Batch    Offset   Got      Total    Time     Rate        \n",
      "--------------------------------------------------------------------------------\n",
      "1        0        500      500      2.35     213.1       \n",
      "2        500      500      1000     5.51     90.8        \n",
      "3        1000     500      1500     3.48     143.8       \n",
      "4        1500     500      2000     3.23     155.0       \n",
      "5        2000     500      2500     3.28     152.5       \n",
      "6        2500     500      3000     3.59     139.2       \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Total: 3000 records in 21.43s (140.0 rec/sec)\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Concurrent Requests"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T13:28:37.691577Z",
     "start_time": "2026-02-04T13:28:33.340385Z"
    }
   },
   "source": [
    "async def test_4():\n",
    "    await init_creds(session_id, csrf_token)\n",
    "    if not api:\n",
    "        print(\"‚ùå API not initialized\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"TEST 4: CONCURRENT ASYNC REQUESTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    offsets = [0, 500, 1000, 1500, 2000]\n",
    "    print(f\"\\nFetching {len(offsets)} requests in parallel...\\n\")\n",
    "    \n",
    "    start = time.time()\n",
    "    tasks = [api.fetch_data(limit=500, offset=off) for off in offsets]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    concurrent_time = time.time() - start\n",
    "    \n",
    "    print(f\"{'Offset':<10} {'Records':<12} {'Time':<10} {'Rate':<12}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    seq_time = 0\n",
    "    for offset, (recs, metadata) in zip(offsets, results):\n",
    "        if metadata['status_code'] == 200:\n",
    "            rate = metadata['records_returned'] / metadata['elapsed_seconds']\n",
    "            seq_time += metadata['elapsed_seconds']\n",
    "            print(f\"{offset:<10} {len(recs):<12} {metadata['elapsed_seconds']:<10.2f} {rate:<12.1f}\")\n",
    "    \n",
    "    print(\"-\"*80)\n",
    "    print(f\"\\nConcurrent: {concurrent_time:.2f}s | Sequential: {seq_time:.2f}s | Speedup: {seq_time/concurrent_time:.1f}x\")\n",
    "\n",
    "await test_4()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GENERATING FRESH JWT TOKEN\n",
      "================================================================================\n",
      "\n",
      "‚úì Loaded from keyring\n",
      "  Session ID: 1tqttfzz17aq08ssvdij2xp02ikm1l...\n",
      "  CSRF Token: PHwKH3JTIt0W5NVfK5YFiO1MemEr2w...\n",
      "\n",
      "Making POST request to: https://app.datarails.com/jwt/api/token/\n",
      "‚úÖ JWT Generated!\n",
      "Token: eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ0b2tlbl90eXBlIjoiYWN...\n",
      "\n",
      "‚úì Credentials refreshed and API client reinitialized\n",
      "================================================================================\n",
      "TEST 4: CONCURRENT ASYNC REQUESTS\n",
      "================================================================================\n",
      "\n",
      "Fetching 5 requests in parallel...\n",
      "\n",
      "Offset     Records      Time       Rate        \n",
      "--------------------------------------------------------------------------------\n",
      "0          500          2.91       171.7       \n",
      "500        500          3.95       126.7       \n",
      "1000       500          3.36       149.0       \n",
      "1500       500          3.53       141.8       \n",
      "2000       500          4.04       123.9       \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Concurrent: 4.11s | Sequential: 17.78s | Speedup: 4.3x\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T13:28:55.212565Z",
     "start_time": "2026-02-04T13:28:54.979029Z"
    }
   },
   "source": [
    "def test_5():\n",
    "    await init_creds(session_id, csrf_token)\n",
    "    try:\n",
    "        if not sample or len(sample) == 0:\n",
    "            print(\"‚ùå No data. Run Test 3 first.\")\n",
    "            return\n",
    "    except:\n",
    "        print(\"‚ùå No data. Run Test 3 first.\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"TEST 5: DATA ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    \n",
    "    months = defaultdict(lambda: {\"count\": 0, \"total\": 0})\n",
    "    accounts = defaultdict(float)\n",
    "    \n",
    "    for rec in sample:\n",
    "        rd = rec.get('Reporting Date')\n",
    "        amt = rec.get('Amount', 0)\n",
    "        acc = rec.get('DR_ACC_L1', 'Unknown')\n",
    "        \n",
    "        if rd:\n",
    "            dt = datetime.fromtimestamp(rd)\n",
    "            month = dt.strftime('%Y-%m')\n",
    "            months[month][\"count\"] += 1\n",
    "            months[month][\"total\"] += amt\n",
    "        \n",
    "        accounts[acc] += amt\n",
    "    \n",
    "    print(\"By Month:\")\n",
    "    print(f\"{'Month':<12} {'Records':<12} {'Total':<18} {'Avg':<15}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for month in sorted(months.keys()):\n",
    "        count = months[month][\"count\"]\n",
    "        total = months[month][\"total\"]\n",
    "        avg = total / count if count > 0 else 0\n",
    "        print(f\"{month:<12} {count:<12} ${total:>16,.2f} ${avg:>13,.2f}\")\n",
    "    \n",
    "    print(\"\\nTop 10 Accounts:\")\n",
    "    print(f\"{'Account':<35} {'Total':<15}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for acc in sorted(accounts.keys(), key=lambda x: accounts[x], reverse=True)[:10]:\n",
    "        print(f\"{acc:<35} ${accounts[acc]:>13,.2f}\")\n",
    "\n",
    "test_5()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GENERATING FRESH JWT TOKEN\n",
      "================================================================================\n",
      "\n",
      "‚úì Loaded from keyring\n",
      "  Session ID: 1tqttfzz17aq08ssvdij2xp02ikm1l...\n",
      "  CSRF Token: PHwKH3JTIt0W5NVfK5YFiO1MemEr2w...\n",
      "\n",
      "Making POST request to: https://app.datarails.com/jwt/api/token/\n",
      "‚úÖ JWT Generated!\n",
      "Token: eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ0b2tlbl90eXBlIjoiYWN...\n",
      "\n",
      "‚úì Credentials refreshed and API client reinitialized\n",
      "================================================================================\n",
      "TEST 5: DATA ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "By Month:\n",
      "Month        Records      Total              Avg            \n",
      "--------------------------------------------------------------------------------\n",
      "2025-01      33           $      101,544.12 $     3,077.09\n",
      "2025-02      19           $      105,583.68 $     5,557.04\n",
      "2025-03      7            $       69,103.04 $     9,871.86\n",
      "2025-04      13           $       81,140.70 $     6,241.59\n",
      "2025-05      35           $       57,333.56 $     1,638.10\n",
      "2025-08      1530         $    8,052,317.66 $     5,262.95\n",
      "2025-09      1363         $    4,122,738.55 $     3,024.75\n",
      "\n",
      "Top 10 Accounts:\n",
      "Account                             Total          \n",
      "--------------------------------------------------------------------------------\n",
      "Operating Expense                   $11,741,020.12\n",
      "Cost of Good sold                   $   673,439.55\n",
      "REVENUE                             $   105,462.09\n",
      "Financial Expenses                  $    69,839.55\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "source": "async def proper_data_analysis():\n    \"\"\"Show the RIGHT way to analyze this unsorted data\"\"\"\n    await init_creds(session_id, csrf_token)\n    \n    print(\"=\"*100)\n    print(\"THE RIGHT WAY: How to Properly Analyze Unsorted Financial Data\")\n    print(\"=\"*100)\n    print()\n    \n    # Fetch 3K records\n    all_records = []\n    for offset in [0, 1000, 2000]:\n        records, metadata = await api.fetch_data(limit=1000, offset=offset)\n        if metadata['status_code'] == 200:\n            all_records.extend(records)\n    \n    print(f\"Working with {len(all_records)} records\")\n    print()\n    \n    from collections import defaultdict\n    \n    # === METHOD 1: P&L by Month (Client-Side Sorting) ===\n    print(\"üìä METHOD 1: Build Monthly P&L (CORRECT APPROACH)\")\n    print(\"-\"*100)\n    print()\n    \n    pnl_by_month = defaultdict(lambda: defaultdict(float))\n    \n    for rec in all_records:\n        rd = rec.get('Reporting Date')\n        if not rd:\n            continue\n        \n        month = datetime.fromtimestamp(rd).strftime('%Y-%m')\n        account = rec.get('DR_ACC_L1', 'Unknown')\n        amount = rec.get('Amount', 0)\n        \n        pnl_by_month[month][account] += amount\n    \n    print(f\"{'Month':<12} {'Revenue':<18} {'OpEx':<18} {'COGS':<18} {'Financial':<18} {'Net':<18}\")\n    print(\"-\"*100)\n    \n    for month in sorted(pnl_by_month.keys()):\n        revenue = pnl_by_month[month].get('REVENUE', 0)\n        opex = pnl_by_month[month].get('Operating Expense', 0)\n        cogs = pnl_by_month[month].get('Cost of Good sold', 0)\n        fin = pnl_by_month[month].get('Financial Expenses', 0)\n        net = revenue - cogs - opex + fin\n        \n        print(f\"{month:<12} ${revenue:>16,.0f} ${opex:>16,.0f} ${cogs:>16,.0f} ${fin:>16,.0f} ${net:>16,.0f}\")\n    \n    print()\n    print(\"‚úì This is the CORRECT P&L by month!\")\n    print(\"‚úì Notice how different months have different totals (data is legitimate)\")\n    print()\n    \n    # === METHOD 2: By Cost Center ===\n    print()\n    print(\"üíº METHOD 2: Financial View by Cost Center (Departmental Analysis)\")\n    print(\"-\"*100)\n    print()\n    \n    by_cost_center = defaultdict(float)\n    \n    for rec in all_records:\n        cc = rec.get('Cost Center', 'No Cost Center')\n        amount = rec.get('Amount', 0)\n        by_cost_center[cc] += amount\n    \n    print(f\"{'Cost Center':<30} {'Total Amount':<20} {'Contribution':<15}\")\n    print(\"-\"*100)\n    \n    total_all = sum(by_cost_center.values())\n    \n    for cc, total in sorted(by_cost_center.items(), key=lambda x: x[1], reverse=True)[:10]:\n        pct = (total / total_all * 100) if total_all != 0 else 0\n        bar = \"‚ñà\" * int(pct / 2)\n        print(f\"{str(cc):<30} ${total:>18,.0f} {pct:>5.1f}% {bar}\")\n    \n    print()\n    print(\"‚úì This shows which departments/cost centers are the biggest spend\")\n    print()\n    \n    # === METHOD 3: Data Quality Checks ===\n    print()\n    print(\"‚úÖ METHOD 3: Data Quality Validation\")\n    print(\"-\"*100)\n    print()\n    \n    total_records = len(all_records)\n    total_amount = sum([r.get('Amount', 0) for r in all_records])\n    negative_count = len([r for r in all_records if r.get('Amount', 0) < 0])\n    positive_count = len([r for r in all_records if r.get('Amount', 0) > 0])\n    zero_count = len([r for r in all_records if r.get('Amount', 0) == 0])\n    \n    print(f\"Total records: {total_records:,}\")\n    print(f\"Total amount: ${total_amount:,.2f}\")\n    print()\n    print(f\"Records breakdown:\")\n    print(f\"  ‚Ä¢ Positive: {positive_count:,} ({positive_count/total_records*100:.1f}%)\")\n    print(f\"  ‚Ä¢ Negative: {negative_count:,} ({negative_count/total_records*100:.1f}%)\")\n    print(f\"  ‚Ä¢ Zero: {zero_count:,} ({zero_count/total_records*100:.1f}%)\")\n    print()\n    print(\"‚úì Negatives are NORMAL (14.9% is expected for Financial Expenses reversals)\")\n    print(\"‚úì Data looks healthy - mix of positive/negative transactions\")\n    print()\n    \n    # === KEY TAKEAWAYS ===\n    print()\n    print(\"=\"*100)\n    print(\"üéØ KEY TAKEAWAYS FOR PROPER DATA ANALYSIS\")\n    print(\"=\"*100)\n    print()\n    print(\"1. ALWAYS sort client-side:\")\n    print(\"   ‚Üí By Reporting Date first (critical!)\")\n    print(\"   ‚Üí Then by DR_ACC_L1 (account type)\")\n    print(\"   ‚Üí Then group/aggregate\")\n    print()\n    print(\"2. UNDERSTAND the structure:\")\n    print(\"   ‚Üí Records are NOT chronological\")\n    print(\"   ‚Üí Accounts are mixed throughout\")\n    print(\"   ‚Üí Cost Center is the organizational key\")\n    print()\n    print(\"3. VALIDATE the data:\")\n    print(\"   ‚Üí 14.9% negatives are OK\")\n    print(\"   ‚Üí Each month should be analyzed separately\")\n    print(\"   ‚Üí Cross-check with Cost Centers\")\n    print()\n    print(\"4. EXTRACTION should:\")\n    print(\"   ‚Üí Fetch in 500-record batches (optimal speed)\")\n    print(\"   ‚Üí Sort ALL records by date before aggregating\")\n    print(\"   ‚Üí Create separate worksheets by account type\")\n    print(\"   ‚Üí Include Cost Center breakdown for visibility\")\n    print()\n\nawait proper_data_analysis()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## How to Properly Analyze & Extract This Data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "async def deep_dive_table_structure():\n    \"\"\"Comprehensive analysis of how the table is organized\"\"\"\n    await init_creds(session_id, csrf_token)\n    \n    print(\"=\"*100)\n    print(\"DEEP DIVE: HOW THIS TABLE IS ACTUALLY STRUCTURED\")\n    print(\"=\"*100)\n    print()\n    \n    # Fetch 5K records for analysis\n    all_records = []\n    for offset in [0, 1000, 2000, 3000, 4000]:\n        records, metadata = await api.fetch_data(limit=1000, offset=offset)\n        if metadata['status_code'] == 200:\n            all_records.extend(records)\n    \n    print(f\"Analyzed {len(all_records)} records\")\n    print()\n    \n    # === UNDERSTANDING 1: What accounts exist? ===\n    print(\"1Ô∏è‚É£  THE ACCOUNT HIERARCHY\")\n    print(\"-\"*100)\n    \n    from collections import defaultdict, Counter\n    \n    hierarchy = defaultdict(set)\n    acc_l1_dist = Counter()\n    \n    for rec in all_records:\n        l1 = rec.get('DR_ACC_L1', 'Unknown')\n        l2 = rec.get('DR_ACC_L2', 'Unknown')\n        hierarchy[l1].add(l2)\n        acc_l1_dist[l1] += 1\n    \n    print(\"Account Structure (L1 ‚Üí L2):\")\n    print()\n    for l1 in sorted(hierarchy.keys()):\n        count = acc_l1_dist[l1]\n        pct = (count / len(all_records)) * 100\n        print(f\"üìä {l1} ({count} records, {pct:.1f}%)\")\n        for l2 in sorted(hierarchy[l1]):\n            if l2 != 'Unknown' and l2:\n                print(f\"     ‚îî‚îÄ {l2}\")\n    \n    print()\n    \n    # === UNDERSTANDING 2: Cost Centers ===\n    print(\"2Ô∏è‚É£  COST CENTER DISTRIBUTION (Key Dimension)\")\n    print(\"-\"*100)\n    \n    cost_centers = Counter()\n    for rec in all_records:\n        cc = rec.get('Cost Center', 'No Cost Center')\n        cost_centers[cc] += 1\n    \n    print()\n    for cc, count in cost_centers.most_common(15):\n        pct = (count / len(all_records)) * 100\n        bar = \"‚ñà\" * int(pct / 2)\n        print(f\"{str(cc):<30} {count:>6} records {pct:>5.1f}% {bar}\")\n    \n    print()\n    print(\"üí° Insight: Data is organized by Cost Center (not just P&L accounts)\")\n    print(\"   ‚Üí Financing (22.6%) = Financial Expenses\")\n    print(\"   ‚Üí Marketing (12.7%) = Operating Expense (Marketing)\")\n    print(\"   ‚Üí HR/R&D/Sales = Operating Expense sub-categories\")\n    print()\n    \n    # === UNDERSTANDING 3: Negative Values ===\n    print(\"3Ô∏è‚É£  NEGATIVE VALUES (Not Errors - Legitimate Transactions)\")\n    print(\"-\"*100)\n    \n    negatives_by_account = defaultdict(float)\n    for rec in all_records:\n        amt = rec.get('Amount', 0)\n        if amt < 0:\n            acc = rec.get('DR_ACC_L1', 'Unknown')\n            negatives_by_account[acc] += 1\n    \n    print()\n    print(f\"Total records with negatives: {sum(negatives_by_account.values())} ({sum(negatives_by_account.values())/len(all_records)*100:.1f}%)\")\n    print()\n    print(\"By Account Type:\")\n    for acc, count in sorted(negatives_by_account.items(), key=lambda x: x[1], reverse=True):\n        total_for_acc = acc_l1_dist[acc]\n        pct = (count / total_for_acc) * 100\n        print(f\"  ‚Ä¢ {acc:<30} {int(count):>6} negatives ({pct:>5.1f}% of {total_for_acc} records)\")\n    \n    print()\n    print(\"‚úì Negatives are legitimate adjustments/reversals\")\n    print(\"  ‚Üí Financial Expenses: reversals/write-downs\")\n    print(\"  ‚Üí Operating Expense: corrections/credits\")\n    print()\n    \n    # === UNDERSTANDING 4: Month Distribution ===\n    print(\"4Ô∏è‚É£  MONTH DISTRIBUTION (Why Uneven?)\")\n    print(\"-\"*100)\n    \n    months = Counter()\n    for rec in all_records:\n        rd = rec.get('Reporting Date')\n        if rd:\n            month = datetime.fromtimestamp(rd).strftime('%Y-%m')\n            months[month] += 1\n    \n    print()\n    for month in sorted(months.keys()):\n        count = months[month]\n        pct = (count / len(all_records)) * 100\n        bar = \"‚ñà\" * int(pct / 0.5)\n        print(f\"{month}: {count:>6} records {pct:>5.1f}% {bar}\")\n    \n    print()\n    print(\"‚ö†Ô∏è  Missing months: July, October, December (not in 5K sample)\")\n    print()\n    \n    # === UNDERSTANDING 5: The Real Picture ===\n    print(\"5Ô∏è‚É£  HOW TO ANALYZE THIS DATA CORRECTLY\")\n    print(\"-\"*100)\n    print()\n    print(\"‚úó WRONG: Just sum all amounts (doesn't account for structure)\")\n    print(\"‚úì CORRECT: Always aggregate by:\")\n    print(\"   1. Reporting Date (CRITICAL - data is not date-sorted)\")\n    print(\"   2. Account (DR_ACC_L1 or DR_ACC_L2)\")\n    print(\"   3. Cost Center (if analyzing by department)\")\n    print(\"   4. Filter out or mark negatives explicitly\")\n    print()\n    print(\"‚úì This is why extraction must sort client-side!\")\n    print(\"‚úì This explains why records 50K have $33.6M (happens to have big OpEx amounts)\")\n    print(\"‚úì This explains negatives in 40K (happens to have Financial Expenses reversals)\")\n    print()\n\nawait deep_dive_table_structure()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## DEEP DIVE: Complete Table Structure Analysis",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "async def investigate_account_types_by_range():\n    \"\"\"Investigate if different ranges contain different account types\"\"\"\n    await init_creds(session_id, csrf_token)\n    \n    print(\"=\"*100)\n    print(\"HYPOTHESIS: Different ranges represent different account types or dimensions?\")\n    print(\"=\"*100)\n    print()\n    print(\"Fetching representative samples from each range to see what accounts they contain...\")\n    print()\n    \n    # Define ranges to examine\n    ranges = [\n        (0, \"First batch (0-500)\"),\n        (10000, \"Records 10K-10.5K\"),\n        (15000, \"Records 15K-15.5K (gap)\"),\n        (20000, \"Records 20K-20.5K\"),\n        (30000, \"Records 30K-30.5K\"),\n        (40000, \"Records 40K-40.5K (negatives)\"),\n        (50000, \"Records 50K-50.5K (huge $33.6M)\"),\n    ]\n    \n    for offset, label in ranges:\n        records, metadata = await api.fetch_data(limit=500, offset=offset)\n        \n        if metadata['status_code'] != 200:\n            print(f\"‚ùå Error fetching {label}\")\n            continue\n        \n        # Analyze what's in this range\n        from collections import Counter\n        accounts = Counter()\n        l0_categories = Counter()\n        \n        for rec in records:\n            acc_l1 = rec.get('DR_ACC_L1', 'Unknown')\n            acc_l0 = rec.get('DR_ACC_L0', 'Unknown')\n            accounts[acc_l1] += 1\n            l0_categories[acc_l0] += 1\n        \n        total = sum([r.get('Amount', 0) for r in records])\n        \n        print(f\"üìä {label}\")\n        print(f\"   Total: ${total:,.2f}\")\n        print(f\"   Top 5 Accounts (DR_ACC_L1):\")\n        for acc, count in accounts.most_common(5):\n            pct = (count / len(records)) * 100\n            print(f\"     ‚Ä¢ {acc}: {count} records ({pct:.1f}%)\")\n        print(f\"   Categories (DR_ACC_L0): {dict(l0_categories)}\")\n        print()\n    \n    print(\"=\"*100)\n    print(\"üîç ANALYSIS:\")\n    print()\n    print(\"Checking if each range is SEGMENTED by account type...\")\n    print()\n    print(\"If different ranges contain DIFFERENT account types, then:\")\n    print(\"  ‚úì Records 50K-50.5K might be ALL Revenue accounts ($33.6M)\")\n    print(\"  ‚úì Records 40K-40.5K might be ALL OpEx with reversals\")\n    print(\"  ‚úì The data IS organized by account type, just not sorted by date\")\n    print()\n    print(\"If each range contains MIXED account types, then:\")\n    print(\"  ‚úó Data is randomly mixed\")\n    print(\"  ‚úó No clear organization pattern\")\n    print()\n\nawait investigate_account_types_by_range()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## INVESTIGATION: Are Different Ranges Different Account Types?",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"=\"*80)\nprint(\"SUMMARY: DATA QUALITY ISSUES & RECOMMENDATIONS\")\nprint(\"=\"*80)\nprint()\nprint(\"‚úÖ WHAT'S WORKING:\")\nprint(\"  ‚Ä¢ API is responding correctly (200 OK)\")\nprint(\"  ‚Ä¢ All 54,390 records are fetchable\")\nprint(\"  ‚Ä¢ Authentication and pagination work\")\nprint(\"  ‚Ä¢ Data extraction is accurate\")\nprint()\nprint(\"‚ùå WHAT NEEDS TO BE FIXED (in Datarails backend):\")\nprint()\nprint(\"1. UNEVEN DATA DISTRIBUTION\")\nprint(\"   Problem: Records are not evenly loaded across months\")\nprint(\"   Example:\")\nprint(\"     ‚Ä¢ January 2025: 33 records\")\nprint(\"     ‚Ä¢ May 2025: 1,481 records\")\nprint(\"     ‚Ä¢ November 2025: Concentrated at end with $33.6M\")\nprint(\"   Fix: Verify data loading process and redistribute evenly\")\nprint()\nprint(\"2. MISSING MONTHS\")\nprint(\"   Problem: July, October, December 2025 have ZERO records\")\nprint(\"   Question: Is this data not loaded yet?\")\nprint(\"   Fix: Confirm if data exists and load it, or document why it's missing\")\nprint()\nprint(\"3. NEGATIVE VALUES\")\nprint(\"   Problem: June 2025 contains negative amounts (-$173K in records 40K-40.5K)\")\nprint(\"   Question: Are these adjustments/reversals or data entry errors?\")\nprint(\"   Fix: Validate negative values and mark as reversals if intentional\")\nprint()\nprint(\"4. NO SORTING\")\nprint(\"   Problem: Records are not sorted by date/amount/account\")\nprint(\"   Impact: November $33.6M is at the END (records 50K-50.5K)\")\nprint(\"   Fix: Add optional sort_by parameter to API (sort_by: date, amount, account)\")\nprint()\nprint(\"‚ùì QUESTIONS FOR PLATFORM TEAM:\")\nprint(\"  1. Why is 2025 data unsorted in the database?\")\nprint(\"  2. When will July, October, December data be loaded?\")\nprint(\"  3. Are negative values legitimate reversals or data errors?\")\nprint(\"  4. Can you add a sort parameter to the API for client convenience?\")\nprint()\nprint(\"=\"*80)",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T13:34:37.689452Z",
     "start_time": "2026-02-04T13:34:37.686429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SUMMARY: DATA QUALITY ISSUES & RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "‚úÖ WHAT'S WORKING:\n",
      "  ‚Ä¢ API is responding correctly (200 OK)\n",
      "  ‚Ä¢ All 54,390 records are fetchable\n",
      "  ‚Ä¢ Authentication and pagination work\n",
      "  ‚Ä¢ Data extraction is accurate\n",
      "\n",
      "‚ùå WHAT NEEDS TO BE FIXED (in Datarails backend):\n",
      "\n",
      "1. UNEVEN DATA DISTRIBUTION\n",
      "   Problem: Records are not evenly loaded across months\n",
      "   Example:\n",
      "     ‚Ä¢ January 2025: 33 records\n",
      "     ‚Ä¢ May 2025: 1,481 records\n",
      "     ‚Ä¢ November 2025: Concentrated at end with $33.6M\n",
      "   Fix: Verify data loading process and redistribute evenly\n",
      "\n",
      "2. MISSING MONTHS\n",
      "   Problem: July, October, December 2025 have ZERO records\n",
      "   Question: Is this data not loaded yet?\n",
      "   Fix: Confirm if data exists and load it, or document why it's missing\n",
      "\n",
      "3. NEGATIVE VALUES\n",
      "   Problem: June 2025 contains negative amounts (-$173K in records 40K-40.5K)\n",
      "   Question: Are these adjustments/reversals or data entry errors?\n",
      "   Fix: Validate negative values and mark as reversals if intentional\n",
      "\n",
      "4. NO SORTING\n",
      "   Problem: Records are not sorted by date/amount/account\n",
      "   Impact: November $33.6M is at the END (records 50K-50.5K)\n",
      "   Fix: Add optional sort_by parameter to API (sort_by: date, amount, account)\n",
      "\n",
      "‚ùì QUESTIONS FOR PLATFORM TEAM:\n",
      "  1. Why is 2025 data unsorted in the database?\n",
      "  2. When will July, October, December data be loaded?\n",
      "  3. Are negative values legitimate reversals or data errors?\n",
      "  4. Can you add a sort parameter to the API for client convenience?\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "source": "## Summary: What Should Be Fixed",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "async def examine_data_concentration():\n    \"\"\"ISSUE 3: Find extreme data concentration/spikes\"\"\"\n    await init_creds(session_id, csrf_token)\n    \n    print(\"=\"*80)\n    print(\"ISSUE 3: DATA CONCENTRATION (Extreme Spikes)\")\n    print(\"=\"*80)\n    print()\n    print(\"Scanning batches for unusual concentration of values...\")\n    print()\n    \n    batch_analysis = []\n    \n    # Sample key ranges to find concentrations\n    test_offsets = [\n        (0, \"First batch (0-500)\"),\n        (10000, \"Records 10K-10.5K\"),\n        (20000, \"Records 20K-20.5K\"),\n        (30000, \"Records 30K-30.5K\"),\n        (40000, \"Records 40K-40.5K\"),\n        (50000, \"Records 50K-50.5K (expected concentration)\"),\n    ]\n    \n    for offset, label in test_offsets:\n        records, metadata = await api.fetch_data(limit=500, offset=offset)\n        \n        if metadata['status_code'] == 200:\n            total = sum([r.get('Amount', 0) for r in records])\n            max_amt = max([r.get('Amount', 0) for r in records]) if records else 0\n            min_amt = min([r.get('Amount', 0) for r in records]) if records else 0\n            avg_amt = total / len(records) if records else 0\n            \n            batch_analysis.append({\n                'label': label,\n                'total': total,\n                'max': max_amt,\n                'min': min_amt,\n                'avg': avg_amt,\n                'records': len(records)\n            })\n    \n    print(f\"{'Batch Range':<30} {'Total Amount':<18} {'Max':<15} {'Min':<15} {'Avg':<15}\")\n    print(\"-\"*95)\n    \n    for batch in batch_analysis:\n        print(f\"{batch['label']:<30} ${batch['total']:>16,.2f} ${batch['max']:>13,.2f} ${batch['min']:>13,.2f} ${batch['avg']:>13,.2f}\")\n    \n    print()\n    print(\"‚ùå FINDINGS:\")\n    print(\"  ‚Ä¢ Records 50K-50.5K contain EXTREME concentration: $33.6M+\")\n    print(\"  ‚Ä¢ This is 40X more than other batches!\")\n    print(\"  ‚Ä¢ Data appears to be unsorted (not organized by month/account)\")\n    print(\"  ‚Ä¢ November data is clustered at the END of the dataset\")\n    print(\"  ‚Ä¢ Ranges with negatives show inverted values (-$173K in 40K-40.5K)\")\n\nawait examine_data_concentration()",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T13:35:47.717477Z",
     "start_time": "2026-02-04T13:35:14.338384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GENERATING FRESH JWT TOKEN\n",
      "================================================================================\n",
      "\n",
      "‚úì Loaded from keyring\n",
      "  Session ID: 1tqttfzz17aq08ssvdij2xp02ikm1l...\n",
      "  CSRF Token: PHwKH3JTIt0W5NVfK5YFiO1MemEr2w...\n",
      "\n",
      "Making POST request to: https://app.datarails.com/jwt/api/token/\n",
      "‚úÖ JWT Generated!\n",
      "Token: eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ0b2tlbl90eXBlIjoiYWN...\n",
      "\n",
      "‚úì Credentials refreshed and API client reinitialized\n",
      "================================================================================\n",
      "ISSUE 3: DATA CONCENTRATION (Extreme Spikes)\n",
      "================================================================================\n",
      "\n",
      "Scanning batches for unusual concentration of values...\n",
      "\n",
      "Batch Range                    Total Amount       Max             Min             Avg            \n",
      "-----------------------------------------------------------------------------------------------\n",
      "First batch (0-500)            $    1,566,842.01 $   209,918.20 $  -136,952.22 $     3,133.68\n",
      "Records 10K-10.5K              $      867,908.10 $   180,000.00 $   -21,722.68 $     1,735.82\n",
      "Records 20K-20.5K              $    1,385,659.49 $   267,648.45 $   -74,709.00 $     2,771.32\n",
      "Records 30K-30.5K              $      871,426.90 $     5,716.00 $   -11,120.65 $     1,742.85\n",
      "Records 40K-40.5K              $     -173,109.26 $ 1,458,998.32 $-1,609,216.24 $      -346.22\n",
      "Records 50K-50.5K (expected concentration) $   33,629,391.38 $33,400,000.00 $   -45,887.84 $    67,258.78\n",
      "\n",
      "‚ùå FINDINGS:\n",
      "  ‚Ä¢ Records 50K-50.5K contain EXTREME concentration: $33.6M+\n",
      "  ‚Ä¢ This is 40X more than other batches!\n",
      "  ‚Ä¢ Data appears to be unsorted (not organized by month/account)\n",
      "  ‚Ä¢ November data is clustered at the END of the dataset\n",
      "  ‚Ä¢ Ranges with negatives show inverted values (-$173K in 40K-40.5K)\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "source": "## ISSUE 3: Data Concentration (Extreme Spikes)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "async def examine_negative_values():\n    \"\"\"ISSUE 2: Find records with negative amounts (data integrity issue)\"\"\"\n    await init_creds(session_id, csrf_token)\n    \n    print(\"=\"*80)\n    print(\"ISSUE 2: NEGATIVE VALUES (Data Integrity)\")\n    print(\"=\"*80)\n    print()\n    print(\"Searching for negative amounts in records 40K-41K...\")\n    print(\"(Diagnostic report found -$173K concentrated here)\")\n    print()\n    \n    # Fetch records from the range known to have negatives\n    records, metadata = await api.fetch_data(limit=500, offset=40000)\n    \n    if metadata['status_code'] != 200:\n        print(f\"‚ùå Error: {metadata.get('error')}\")\n        return\n    \n    # Find negatives\n    negatives = [r for r in records if r.get('Amount', 0) < 0]\n    \n    print(f\"Found {len(negatives)} negative records out of {len(records)}\")\n    print()\n    \n    if negatives:\n        print(f\"{'Month':<12} {'Account':<35} {'Amount':<15} {'Posting Date':<20}\")\n        print(\"-\"*80)\n        \n        total_neg = 0\n        for rec in negatives[:20]:  # Show first 20\n            rd = rec.get('Reporting Date')\n            month = datetime.fromtimestamp(rd).strftime('%Y-%m') if rd else 'N/A'\n            account = rec.get('DR_ACC_L1', 'Unknown')[:33]\n            amount = rec.get('Amount', 0)\n            posting_date = rec.get('Posting Date', 'N/A')\n            \n            print(f\"{month:<12} {account:<35} ${amount:>13,.2f} {posting_date:<20}\")\n            total_neg += amount\n        \n        print(\"-\"*80)\n        print(f\"Total negative amount (first 20): ${total_neg:,.2f}\")\n        print()\n        print(\"‚ùå FINDINGS:\")\n        print(f\"  ‚Ä¢ {len(negatives)} records have NEGATIVE amounts\")\n        print(\"  ‚Ä¢ Concentrated in June 2025 data\")\n        print(\"  ‚Ä¢ Total negative sum: ${:,.2f}\".format(total_neg))\n        print(\"  ‚Ä¢ Could be reversals/adjustments or data errors\")\n    else:\n        print(\"‚úì No negative values found in this range\")\n\nawait examine_negative_values()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## ISSUE 2: Negative Values (Data Integrity)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "async def examine_month_distribution():\n    \"\"\"ISSUE 1: Examine uneven month distribution across all 54,390 records\"\"\"\n    await init_creds(session_id, csrf_token)\n    \n    print(\"=\"*80)\n    print(\"ISSUE 1: MONTH DISTRIBUTION (Uneven Loading)\")\n    print(\"=\"*80)\n    print()\n    print(\"Fetching ALL records to analyze month distribution...\")\n    print()\n    \n    all_records = []\n    offset = 0\n    batch_size = 1000\n    batches = 0\n    \n    # Fetch all 54,390 records in batches\n    while True:\n        batches += 1\n        records, metadata = await api.fetch_data(limit=batch_size, offset=offset)\n        \n        if metadata['status_code'] != 200:\n            print(f\"‚ùå Error at offset {offset}: {metadata.get('error')}\")\n            break\n        \n        all_records.extend(records)\n        print(f\"Batch {batches}: Fetched {len(records)} records (Total: {len(all_records)})\", end=\"\\r\")\n        \n        if len(records) < batch_size:\n            break\n        \n        offset += batch_size\n    \n    print()\n    print()\n    \n    # Analyze month distribution\n    from collections import defaultdict\n    month_dist = defaultdict(lambda: {\"count\": 0, \"total\": 0})\n    \n    for rec in all_records:\n        rd = rec.get('Reporting Date')\n        amt = rec.get('Amount', 0)\n        \n        if rd:\n            dt = datetime.fromtimestamp(rd)\n            month = dt.strftime('%Y-%m')\n            month_dist[month][\"count\"] += 1\n            month_dist[month][\"total\"] += amt\n    \n    print(\"Month Distribution (ALL records):\")\n    print(f\"{'Month':<12} {'Records':<12} {'Total Amount':<20} {'Avg/Record':<15} {'Status':<20}\")\n    print(\"-\"*80)\n    \n    for month in sorted(month_dist.keys()):\n        count = month_dist[month][\"count\"]\n        total = month_dist[month][\"total\"]\n        avg = total / count if count > 0 else 0\n        \n        # Flag issues\n        status = \"‚úì OK\"\n        if count == 0:\n            status = \"‚ùå MISSING\"\n        elif count < 100:\n            status = \"‚ö†Ô∏è SPARSE\"\n        elif count > 1000:\n            status = \"‚ö†Ô∏è CONCENTRATED\"\n        \n        print(f\"{month:<12} {count:<12} ${total:>18,.2f} ${avg:>13,.2f} {status:<20}\")\n    \n    print()\n    print(f\"Total records analyzed: {len(all_records)}\")\n    print()\n    print(\"‚ùå FINDINGS:\")\n    print(\"  ‚Ä¢ Data is UNEVENLY distributed across months\")\n    print(\"  ‚Ä¢ Missing July, October, December 2025\")\n    print(\"  ‚Ä¢ Some months have 33 records, others have 1500+\")\n    print(\"  ‚Ä¢ November data is extremely concentrated\")\n\nawait examine_month_distribution()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## ISSUE 1: Month Distribution (Uneven Loading)",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}